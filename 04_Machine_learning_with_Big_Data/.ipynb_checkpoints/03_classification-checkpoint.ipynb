{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import DataFrameNaFunctions\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.feature import Binarizer\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, VectorIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/10 17:28:21 WARN Utils: Your hostname, ubuntu resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "23/04/10 17:28:21 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/10 17:28:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hadoop/anaconda3/lib/python3.10/site-packages/pyspark/sql/context.py:112: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['number',\n",
       " 'air_pressure_9am',\n",
       " 'air_temp_9am',\n",
       " 'avg_wind_direction_9am',\n",
       " 'avg_wind_speed_9am',\n",
       " 'max_wind_direction_9am',\n",
       " 'max_wind_speed_9am',\n",
       " 'rain_accumulation_9am',\n",
       " 'rain_duration_9am',\n",
       " 'relative_humidity_9am',\n",
       " 'relative_humidity_3pm']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = pyspark.SparkContext()\n",
    "\n",
    "sqlContext = SQLContext(sc)\n",
    "df = sqlContext.read.load('file:///home/hadoop/Downloads/big-data-4/daily_weather.csv', \n",
    "                          format='com.databricks.spark.csv', \n",
    "                          header='true',inferSchema='true')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " columns in the weather data we will use for the decision tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureColumns = ['air_pressure_9am','air_temp_9am','avg_wind_direction_9am','avg_wind_speed_9am',\n",
    "        'max_wind_direction_9am','max_wind_speed_9am','rain_accumulation_9am',\n",
    "        'rain_duration_9am']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.na.drop() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1064, 10)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count(), len(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create categorical variable.** We create a categorical variable to denote if the humidity is not low. If the value is less than 25%, then we want the categorical value to be 0, otherwise the categorical value should be 1. We create this categorical variable as a column in a DataFrame using Binarizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "binarizer = Binarizer(threshold=24.99999, inputCol=\"relative_humidity_3pm\", outputCol=\"label\")\n",
    "binarizedDF = binarizer.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `threshold` argument specifies the threshold value for the variable, `inputCol` is the input column to read, and `outputCol` is the name of the new categorical column. The second line applies the `Binarizer` and creates a new DataFrame with the categorical column. We can look at the first four values in the new DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-----+\n",
      "|relative_humidity_3pm|label|\n",
      "+---------------------+-----+\n",
      "|   36.160000000000494|  1.0|\n",
      "|     19.4265967985621|  0.0|\n",
      "|   14.460000000000045|  0.0|\n",
      "|   12.742547353761848|  0.0|\n",
      "|    76.74000000000046|  1.0|\n",
      "|   33.930000000000256|  1.0|\n",
      "|   21.385656725200974|  0.0|\n",
      "|    74.92000000000041|  1.0|\n",
      "|   24.030000000000427|  0.0|\n",
      "|     68.0500000000012|  1.0|\n",
      "|    32.13000000000024|  1.0|\n",
      "|     79.0900000000002|  1.0|\n",
      "|    58.43000000000119|  1.0|\n",
      "|   27.990000000000173|  1.0|\n",
      "|   24.369999999999948|  0.0|\n",
      "|   14.801705962979918|  0.0|\n",
      "|    20.75568332171184|  0.0|\n",
      "|    45.87000000000005|  1.0|\n",
      "|    7.740000000000088|  0.0|\n",
      "|   14.649909361535952|  0.0|\n",
      "+---------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "binarizedDF.select(\"relative_humidity_3pm\",\"label\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first row's humidity value is greater than 25% and the label is 1. The other humidity values are less than 25% and have labels equal to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aggregate features.** Let's aggregate the features we will use to make predictions into a single column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=featureColumns, outputCol=\"features\")\n",
    "assembled = assembler.transform(binarizedDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `inputCols` argument specifies our list of column names we defined earlier, and `outputCol` is the name of the new column. The second line creates a new DataFrame with the aggregated features in a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+----------------------+------------------+----------------------+------------------+---------------------+-----------------+---------------------+---------------------+-----+--------------------+\n",
      "| air_pressure_9am|     air_temp_9am|avg_wind_direction_9am|avg_wind_speed_9am|max_wind_direction_9am|max_wind_speed_9am|rain_accumulation_9am|rain_duration_9am|relative_humidity_9am|relative_humidity_3pm|label|            features|\n",
      "+-----------------+-----------------+----------------------+------------------+----------------------+------------------+---------------------+-----------------+---------------------+---------------------+-----+--------------------+\n",
      "|918.0600000000087|74.82200000000041|                 271.1| 2.080354199999768|    295.39999999999986| 2.863283199999908|                  0.0|              0.0|    42.42000000000046|   36.160000000000494|  1.0|[918.060000000008...|\n",
      "+-----------------+-----------------+----------------------+------------------+----------------------+------------------+---------------------+-----------------+---------------------+---------------------+-----+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assembled.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 7:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>air_pressure_9am</th>\n",
       "      <th>air_temp_9am</th>\n",
       "      <th>avg_wind_direction_9am</th>\n",
       "      <th>avg_wind_speed_9am</th>\n",
       "      <th>max_wind_direction_9am</th>\n",
       "      <th>max_wind_speed_9am</th>\n",
       "      <th>rain_accumulation_9am</th>\n",
       "      <th>rain_duration_9am</th>\n",
       "      <th>relative_humidity_9am</th>\n",
       "      <th>relative_humidity_3pm</th>\n",
       "      <th>label</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>918.060000</td>\n",
       "      <td>74.822000</td>\n",
       "      <td>271.100000</td>\n",
       "      <td>2.080354</td>\n",
       "      <td>295.400000</td>\n",
       "      <td>2.863283</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.420000</td>\n",
       "      <td>36.160000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[918.0600000000087, 74.82200000000041, 271.1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>917.347688</td>\n",
       "      <td>71.403843</td>\n",
       "      <td>101.935179</td>\n",
       "      <td>2.443009</td>\n",
       "      <td>140.471548</td>\n",
       "      <td>3.533324</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.328697</td>\n",
       "      <td>19.426597</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[917.3476881177097, 71.40384263106537, 101.935...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   air_pressure_9am  air_temp_9am  avg_wind_direction_9am  avg_wind_speed_9am  \\\n",
       "0        918.060000     74.822000              271.100000            2.080354   \n",
       "1        917.347688     71.403843              101.935179            2.443009   \n",
       "\n",
       "   max_wind_direction_9am  max_wind_speed_9am  rain_accumulation_9am  \\\n",
       "0              295.400000            2.863283                    0.0   \n",
       "1              140.471548            3.533324                    0.0   \n",
       "\n",
       "   rain_duration_9am  relative_humidity_9am  relative_humidity_3pm  label  \\\n",
       "0                0.0              42.420000              36.160000    1.0   \n",
       "1                0.0              24.328697              19.426597    0.0   \n",
       "\n",
       "                                            features  \n",
       "0  [918.0600000000087, 74.82200000000041, 271.1, ...  \n",
       "1  [917.3476881177097, 71.40384263106537, 101.935...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assembled.toPandas()[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split training and test data by calling `randomSplit()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainingData, testData) = assembled.randomSplit([0.8,0.2], seed = 13234 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(846, 218)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingData.count(), testData.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\", maxDepth=5,\n",
    "                            minInstancesPerNode=20, impurity=\"gini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `labelCol`argument is the column we are trying to predict, \\\n",
    "`featuresCol` specifies the aggregated features column, \\\n",
    "`maxDepth` is stopping criterion for tree induction based on maximum depth of tree, \\\n",
    "`minInstancesPerNode` is stopping criterion for tree induction based on minimum number of samples in a node, and \\\n",
    "`impurity` is the impurity measure used to split nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(stages=[dt])\n",
    "model = pipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|prediction|label|\n",
      "+----------+-----+\n",
      "|       1.0|  1.0|\n",
      "|       1.0|  1.0|\n",
      "|       0.0|  1.0|\n",
      "|       1.0|  1.0|\n",
      "|       1.0|  1.0|\n",
      "|       1.0|  1.0|\n",
      "|       1.0|  1.0|\n",
      "|       1.0|  1.0|\n",
      "|       0.0|  0.0|\n",
      "|       1.0|  1.0|\n",
      "|       1.0|  1.0|\n",
      "|       1.0|  1.0|\n",
      "|       1.0|  1.0|\n",
      "|       0.0|  0.0|\n",
      "|       1.0|  1.0|\n",
      "|       1.0|  1.0|\n",
      "|       0.0|  1.0|\n",
      "|       0.0|  1.0|\n",
      "|       0.0|  1.0|\n",
      "|       1.0|  1.0|\n",
      "+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 28:>                                                         (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions.select(\"prediction\", \"label\").show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save the predictions to a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions.select(\"prediction\", \"label\")\\\n",
    "            .write\\\n",
    "            .save(path=\"file:///home/hadoop/big-data-processing/04_Machine_learning_with_Big_Data/predictions.csv\",\n",
    "                                                     format=\"com.databricks.spark.csv\",\n",
    "                                                     header='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
